# VPSie Kubernetes Node Autoscaler - Phase 4: Production Readiness Prompts

## Project Context
The VPSie Kubernetes Node Autoscaler is at v0.3.0-alpha with Phase 1-3 complete. Core functionality exists but critical production features are missing. These prompts provide step-by-step implementation instructions for achieving v1.0.0 production readiness.

**Current Status:**
- ✅ VPSie API client with OAuth authentication
- ✅ NodeGroup and VPSieNode CRDs with controllers
- ✅ Event-driven scale-up based on unschedulable pods
- ✅ 66 unit tests + 16 integration tests + 3 performance tests
- ❌ No scale-down logic (pkg/scaler/ is empty)
- ❌ No deployment manifests (deploy/manifests/ and helm templates empty)
- ❌ 9 hardcoded TODO values throughout codebase
- ❌ No node bootstrapping system

---

## PROMPT 1: Implement Scale-Down Logic (Critical Blocker)

**Task:** Implement complete scale-down functionality in the empty pkg/scaler/ directory to prevent unbounded resource growth and costs.

**Implementation Requirements:**

### 1. Create pkg/scaler/scaler.go
```go
package scaler

import (
    "context"
    "time"
    autoscalerv1alpha1 "github.com/vpsie/vpsie-k8s-autoscaler/pkg/apis/autoscaler/v1alpha1"
    "github.com/vpsie/vpsie-k8s-autoscaler/pkg/metrics"
    corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    metricsv1beta1 "k8s.io/metrics/pkg/client/clientset/versioned"
)

type ScaleDownManager struct {
    client         kubernetes.Interface
    metricsClient  metricsv1beta1.Interface
    nodeUtilization map[string]*NodeUtilization
    cooldownPeriod  time.Duration
    lastScaleDown   time.Time
}

type NodeUtilization struct {
    NodeName          string
    CPUUtilization    float64  // percentage (0-100)
    MemoryUtilization float64  // percentage (0-100)
    Samples           []UtilizationSample
    LastUpdated       time.Time
}

// Core methods to implement:
func (s *ScaleDownManager) IdentifyUnderutilizedNodes(ctx context.Context, nodeGroup *autoscalerv1alpha1.NodeGroup) ([]*corev1.Node, error)
func (s *ScaleDownManager) CanScaleDown(ctx context.Context, node *corev1.Node) (bool, string, error)
func (s *ScaleDownManager) DrainNode(ctx context.Context, node *corev1.Node) error
func (s *ScaleDownManager) ValidatePodDisruptionBudgets(ctx context.Context, pods []*corev1.Pod) error
```

### 2. Create pkg/scaler/utilization.go for metrics collection
```go
// Implement node utilization tracking
func (s *ScaleDownManager) UpdateNodeUtilization(ctx context.Context) error {
    // Query metrics-server for node CPU/memory usage
    // Calculate 5-minute rolling average
    // Store in nodeUtilization map
    // Track utilization trends
}

// Thresholds for scale-down (make configurable):
const (
    ScaleDownCPUThreshold    = 50.0  // Scale down if CPU < 50%
    ScaleDownMemoryThreshold = 50.0  // Scale down if Memory < 50%
    ScaleDownWindow         = 10 * time.Minute  // Observation window
    ScaleDownCooldown       = 10 * time.Minute  // Between scale-downs
)
```

### 3. Create pkg/scaler/drain.go for safe node removal
```go
func (s *ScaleDownManager) DrainNode(ctx context.Context, node *corev1.Node) error {
    // Step 1: Cordon node (mark unschedulable)
    // Step 2: Get all pods on node
    // Step 3: Check PodDisruptionBudgets
    // Step 4: Evict pods with grace period
    // Step 5: Wait for pod termination
    // Step 6: Handle DaemonSets and static pods
    // Step 7: Verify successful migration
    // Step 8: Uncordon if drain fails (safety)
}
```

### 4. Create pkg/scaler/safety.go for safety checks
```go
// Critical safety checks before removal:
func (s *ScaleDownManager) IsSafeToRemove(ctx context.Context, node *corev1.Node) (bool, string, error) {
    // Check 1: Node has no pods with local storage
    // Check 2: All pods can be scheduled elsewhere
    // Check 3: System pods (kube-system) have alternatives
    // Check 4: No pod anti-affinity violations
    // Check 5: Cluster has sufficient capacity after removal
    // Check 6: Node is not annotated as "protected"
    // Check 7: Minimum nodes constraint not violated
}
```

### 5. Integration with NodeGroup Controller
Update pkg/controller/nodegroup/controller.go:
```go
// Add to reconciliation loop:
if shouldScaleDown {
    nodes := scaler.IdentifyUnderutilizedNodes(ctx, nodeGroup)
    for _, node := range nodes {
        if canRemove, reason, err := scaler.CanScaleDown(ctx, node); canRemove {
            // Update VPSieNode to Terminating phase
            // Emit scaling event
            // Update metrics
        }
    }
}
```

### 6. Add comprehensive tests (pkg/scaler/scaler_test.go)
Test scenarios:
- Underutilized node identification
- PDB respect during drain
- Failed eviction handling
- Cooldown period enforcement
- Protected node skipping
- Capacity validation
- Local storage detection

**Expected Files:**
```
pkg/scaler/
├── scaler.go           # Main ScaleDownManager
├── utilization.go      # Metrics collection
├── drain.go           # Node draining logic
├── safety.go          # Safety checks
├── policies.go        # Configurable policies
├── scaler_test.go     # Unit tests
├── utilization_test.go
├── drain_test.go
└── safety_test.go
```

**Success Criteria:**
- ✅ Nodes scale down when CPU/Memory < 50% for 10 minutes
- ✅ PodDisruptionBudgets always respected
- ✅ No data loss from local storage pods
- ✅ Graceful pod migration with retries
- ✅ 90% test coverage
- ✅ Integration with existing controllers

---

## PROMPT 2: Create Production Deployment Manifests

**Task:** Create complete deployment manifests for Helm and kubectl installation methods. Both deploy/manifests/ and deploy/helm/vpsie-autoscaler/templates/ are empty.

### 1. Helm Chart Structure (deploy/helm/vpsie-autoscaler/)

**Chart.yaml:**
```yaml
apiVersion: v2
name: vpsie-autoscaler
description: Kubernetes node autoscaler for VPSie cloud platform
type: application
version: 1.0.0
appVersion: "1.0.0"
keywords:
  - autoscaler
  - vpsie
  - kubernetes
maintainers:
  - name: VPSie Team
    email: support@vpsie.com
```

**values.yaml:**
```yaml
replicaCount: 1  # Use 2+ for HA with leader election

image:
  repository: ghcr.io/vpsie/vpsie-k8s-autoscaler
  pullPolicy: IfNotPresent
  tag: "latest"

controller:
  reconcileInterval: 30s
  leaderElection: true
  metricsPort: 8080
  healthPort: 8081
  logLevel: info

vpsie:
  secretName: vpsie-secret
  secretNamespace: kube-system
  apiRateLimit: 100  # requests per minute

scaling:
  scaleUpCooldown: 60s
  scaleDownCooldown: 600s
  scaleDownUtilizationThreshold: 50
  maxNodesPerScaleUp: 10
  maxNodesPerScaleDown: 5

resources:
  limits:
    cpu: 1000m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

nodeSelector: {}
tolerations: []
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - vpsie-autoscaler
        topologyKey: kubernetes.io/hostname

serviceMonitor:
  enabled: false
  interval: 30s

networkPolicy:
  enabled: false

podDisruptionBudget:
  enabled: true
  minAvailable: 1
```

**templates/deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "vpsie-autoscaler.fullname" . }}
  namespace: {{ .Values.namespace | default "kube-system" }}
  labels:
    {{- include "vpsie-autoscaler.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "vpsie-autoscaler.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "vpsie-autoscaler.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "vpsie-autoscaler.serviceAccountName" . }}
      containers:
      - name: controller
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        args:
          - --metrics-addr=:{{ .Values.controller.metricsPort }}
          - --health-addr=:{{ .Values.controller.healthPort }}
          - --enable-leader-election={{ .Values.controller.leaderElection }}
          - --log-level={{ .Values.controller.logLevel }}
          - --reconcile-interval={{ .Values.controller.reconcileInterval }}
          - --vpsie-secret={{ .Values.vpsie.secretName }}
          - --vpsie-namespace={{ .Values.vpsie.secretNamespace }}
        env:
        - name: CONTROLLER_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - name: metrics
          containerPort: {{ .Values.controller.metricsPort }}
          protocol: TCP
        - name: health
          containerPort: {{ .Values.controller.healthPort }}
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /healthz
            port: health
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: health
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
```

**templates/rbac.yaml:**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "vpsie-autoscaler.serviceAccountName" . }}
  namespace: {{ .Values.namespace | default "kube-system" }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ include "vpsie-autoscaler.fullname" . }}
rules:
# Core resources
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods", "pods/status"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods/eviction"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create", "patch"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
# CRDs
- apiGroups: ["autoscaler.vpsie.com"]
  resources: ["nodegroups", "vpsienodes"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["autoscaler.vpsie.com"]
  resources: ["nodegroups/status", "vpsienodes/status"]
  verbs: ["get", "update", "patch"]
# Metrics
- apiGroups: ["metrics.k8s.io"]
  resources: ["nodes", "pods"]
  verbs: ["get", "list"]
# Policy
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "list", "watch"]
# Leader election
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: {{ include "vpsie-autoscaler.fullname" . }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: {{ include "vpsie-autoscaler.fullname" . }}
subjects:
- kind: ServiceAccount
  name: {{ include "vpsie-autoscaler.serviceAccountName" . }}
  namespace: {{ .Values.namespace | default "kube-system" }}
```

### 2. Kubernetes Manifests (deploy/manifests/)

Create individual YAML files for kubectl users:

**namespace.yaml:**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: vpsie-autoscaler
```

**deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vpsie-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vpsie-autoscaler
  template:
    metadata:
      labels:
        app: vpsie-autoscaler
    spec:
      serviceAccountName: vpsie-autoscaler
      containers:
      - name: controller
        image: ghcr.io/vpsie/vpsie-k8s-autoscaler:latest
        args:
          - --metrics-addr=:8080
          - --health-addr=:8081
          - --enable-leader-election=true
          - --log-level=info
        # ... (similar to Helm template)
```

**kustomization.yaml:**
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - namespace.yaml
  - serviceaccount.yaml
  - clusterrole.yaml
  - clusterrolebinding.yaml
  - configmap.yaml
  - deployment.yaml
  - service.yaml
  - poddisruptionbudget.yaml

configMapGenerator:
  - name: vpsie-autoscaler-config
    files:
      - config.yaml

images:
  - name: ghcr.io/vpsie/vpsie-k8s-autoscaler
    newTag: v1.0.0
```

**Success Criteria:**
- ✅ Helm chart passes `helm lint`
- ✅ Manifests pass `kubectl --dry-run`
- ✅ RBAC permissions minimal but sufficient
- ✅ Resource limits and requests set
- ✅ Health/liveness probes configured
- ✅ Leader election enabled for HA

---

## PROMPT 3: Implement Node Bootstrapping System

**Task:** Create automated system for VPSie VMs to join Kubernetes cluster using cloud-init and kubeadm.

### 1. Create pkg/bootstrap/token_manager.go
```go
package bootstrap

import (
    "context"
    "crypto/rand"
    "encoding/hex"
    "time"

    corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
)

type TokenManager struct {
    client     kubernetes.Interface
    tokenTTL   time.Duration
}

func (t *TokenManager) GenerateBootstrapToken(ctx context.Context) (*BootstrapToken, error) {
    // Generate kubeadm bootstrap token
    // Store in Secret with expiration
    // Return token and CA cert hash
}

func (t *TokenManager) GetClusterEndpoint(ctx context.Context) (string, error) {
    // Get API server endpoint from kubeconfig
}
```

### 2. Create pkg/bootstrap/cloud_init.go
```go
type CloudInitGenerator struct {
    clusterEndpoint string
    caCertHash      string
    templates       map[string]*template.Template
}

func (c *CloudInitGenerator) GenerateUserData(osType string, token string) (string, error) {
    // Generate cloud-init user-data based on OS type
    // Include kubeadm join command with token
}
```

### 3. Create cloud-init templates (pkg/bootstrap/templates/)

**ubuntu-22.04.yaml:**
```yaml
#cloud-config
package_update: true
package_upgrade: true

packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg
  - lsb-release

runcmd:
  # Install Docker
  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  - add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
  - apt-get update && apt-get install -y docker-ce docker-ce-cli containerd.io

  # Install Kubernetes components
  - curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
  - echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
  - apt-get update && apt-get install -y kubelet kubeadm kubectl
  - apt-mark hold kubelet kubeadm kubectl

  # Configure kernel parameters
  - modprobe br_netfilter
  - echo "br_netfilter" >> /etc/modules
  - sysctl -w net.bridge.bridge-nf-call-iptables=1
  - sysctl -w net.ipv4.ip_forward=1
  - echo "net.bridge.bridge-nf-call-iptables=1" >> /etc/sysctl.conf
  - echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf

  # Join cluster
  - |
    kubeadm join {{ .ClusterEndpoint }} \
      --token {{ .Token }} \
      --discovery-token-ca-cert-hash sha256:{{ .CACertHash }} \
      --node-name $(hostname -f)

  # Enable and start kubelet
  - systemctl enable kubelet
  - systemctl start kubelet

  # Label node with VPSie metadata
  - |
    kubectl --kubeconfig /etc/kubernetes/kubelet.conf label node $(hostname -f) \
      vpsie.com/instance-id={{ .InstanceID }} \
      vpsie.com/nodegroup={{ .NodeGroup }} \
      --overwrite

write_files:
  - path: /etc/docker/daemon.json
    content: |
      {
        "exec-opts": ["native.cgroupdriver=systemd"],
        "log-driver": "json-file",
        "log-opts": {
          "max-size": "100m"
        },
        "storage-driver": "overlay2"
      }
```

### 4. Update VPSieNode provisioner
Update pkg/controller/vpsienode/provisioner.go:
```go
func (p *Provisioner) createVM(ctx context.Context, vn *v1alpha1.VPSieNode) error {
    // Generate bootstrap token
    token, err := p.tokenManager.GenerateBootstrapToken(ctx)

    // Generate cloud-init user-data
    userData, err := p.cloudInit.GenerateUserData(vn.Spec.OSImageID, token.Token)

    // Create VPS with user-data
    req := &vpsie.CreateVMRequest{
        Hostname:  vn.Name,
        UserData:  base64.StdEncoding.EncodeToString([]byte(userData)),
        // ... other fields
    }
}
```

### 5. Create validation system (pkg/bootstrap/validator.go)
```go
func (v *Validator) WaitForNodeJoin(ctx context.Context, nodeName string, timeout time.Duration) error {
    // Poll for node to appear in cluster
    // Verify kubelet is healthy
    // Check node Ready condition
    // Validate expected labels/taints
}
```

**Success Criteria:**
- ✅ VMs automatically join cluster within 5 minutes
- ✅ 99.9% bootstrap success rate
- ✅ Support Ubuntu, Debian, CentOS
- ✅ Automatic token rotation
- ✅ Failed join detection and retry
- ✅ Node properly labeled and tainted

---

## PROMPT 4: Fix Configuration Management (9 TODOs)

**Task:** Replace all hardcoded values with proper configuration system using Viper.

### 1. Create internal/config/config.go
```go
package config

import (
    "github.com/spf13/viper"
    "time"
)

type Config struct {
    Controller ControllerConfig `yaml:"controller"`
    VPSie      VPSieConfig     `yaml:"vpsie"`
    Scaling    ScalingConfig   `yaml:"scaling"`
    Bootstrap  BootstrapConfig `yaml:"bootstrap"`
    Metrics    MetricsConfig   `yaml:"metrics"`
    Logging    LoggingConfig   `yaml:"logging"`
}

type ControllerConfig struct {
    ReconcileInterval   time.Duration `yaml:"reconcileInterval"`
    LeaderElection      bool         `yaml:"leaderElection"`
    LeaderElectionID    string       `yaml:"leaderElectionID"`
    MetricsBindAddress  string       `yaml:"metricsBindAddress"`
    HealthProbeAddress  string       `yaml:"healthProbeAddress"`
    Namespace          string       `yaml:"namespace"`
}

type VPSieConfig struct {
    SecretName      string        `yaml:"secretName"`
    SecretNamespace string        `yaml:"secretNamespace"`
    APIRateLimit    int          `yaml:"apiRateLimit"`
    RequestTimeout  time.Duration `yaml:"requestTimeout"`
    RetryAttempts   int          `yaml:"retryAttempts"`
}

type ScalingConfig struct {
    ScaleUpCooldown              time.Duration `yaml:"scaleUpCooldown"`
    ScaleDownCooldown            time.Duration `yaml:"scaleDownCooldown"`
    ScaleDownUtilizationThreshold float64      `yaml:"scaleDownUtilizationThreshold"`
    MaxNodesPerScaleUp           int          `yaml:"maxNodesPerScaleUp"`
    MaxNodesPerScaleDown         int          `yaml:"maxNodesPerScaleDown"`
    NodeStartupTimeout           time.Duration `yaml:"nodeStartupTimeout"`
}

type BootstrapConfig struct {
    CloudInitTemplate string        `yaml:"cloudInitTemplate"`
    SSHKeyIDs        []string      `yaml:"sshKeyIds"`
    DefaultOSImage   string        `yaml:"defaultOSImage"`
    TokenTTL         time.Duration `yaml:"tokenTTL"`
    JoinTimeout      time.Duration `yaml:"joinTimeout"`
}

func Load(configPath string) (*Config, error) {
    viper.SetConfigFile(configPath)
    viper.SetEnvPrefix("VPSIE_AUTOSCALER")
    viper.AutomaticEnv()

    // Set defaults
    viper.SetDefault("controller.reconcileInterval", "30s")
    viper.SetDefault("controller.metricsBindAddress", ":8080")
    viper.SetDefault("scaling.scaleDownUtilizationThreshold", 50.0)
    viper.SetDefault("bootstrap.defaultOSImage", "ubuntu-22.04")

    if err := viper.ReadInConfig(); err != nil {
        return nil, err
    }

    var cfg Config
    if err := viper.Unmarshal(&cfg); err != nil {
        return nil, err
    }

    return &cfg, cfg.Validate()
}

func (c *Config) Validate() error {
    // Validate required fields
    // Check value ranges
    // Verify mutual exclusivity
    return nil
}
```

### 2. Fix TODO items in code

**Fix 1: pkg/controller/manager.go:170-171**
```go
// Before:
"",  // TODO: Make cloud-init template configurable
nil, // TODO: Make SSH key IDs configurable

// After:
cfg.Bootstrap.CloudInitTemplate,
cfg.Bootstrap.SSHKeyIDs,
```

**Fix 2: pkg/controller/vpsienode/provisioner.go:67**
```go
// Before:
OSImageID: "ubuntu-22.04", // TODO: Make configurable from NodeGroup

// After:
OSImageID: nodeGroup.Spec.OSImageID,
// With fallback:
if OSImageID == "" {
    OSImageID = cfg.Bootstrap.DefaultOSImage
}
```

**Fix 3: pkg/controller/vpsienode/provisioner.go:247**
```go
// Before:
// TODO: Replace template variables with actual values

// After:
userData := p.cloudInit.GenerateUserData(CloudInitData{
    ClusterEndpoint: p.config.Controller.ClusterEndpoint,
    Token:          token.Token,
    CACertHash:     token.CACertHash,
    InstanceID:     vm.ID,
    NodeGroup:      nodeGroup.Name,
})
```

**Fix 4: pkg/controller/vpsienode/joiner.go:284**
```go
// Before:
// TODO: Implement label and taint application from NodeGroup spec

// After:
for k, v := range nodeGroup.Spec.Labels {
    node.Labels[k] = v
}
for _, taint := range nodeGroup.Spec.Taints {
    node.Spec.Taints = append(node.Spec.Taints, taint)
}
```

**Fix 5: pkg/controller/events/scaleup.go:160**
```go
// Before:
// TODO: Fetch actual instance type info from VPSie API

// After:
offerings, err := s.vpsieClient.ListOfferings(ctx)
instanceType := s.selectOptimalInstance(offerings, resourceDeficit)
```

### 3. Create config file template (config/config.yaml)
```yaml
# VPSie Kubernetes Autoscaler Configuration
controller:
  reconcileInterval: 30s
  leaderElection: true
  leaderElectionID: vpsie-autoscaler
  metricsBindAddress: :8080
  healthProbeAddress: :8081
  namespace: kube-system

vpsie:
  secretName: vpsie-secret
  secretNamespace: kube-system
  apiRateLimit: 100  # requests per minute
  requestTimeout: 30s
  retryAttempts: 3

scaling:
  scaleUpCooldown: 60s
  scaleDownCooldown: 600s
  scaleDownUtilizationThreshold: 50.0  # percentage
  maxNodesPerScaleUp: 10
  maxNodesPerScaleDown: 5
  nodeStartupTimeout: 10m

bootstrap:
  cloudInitTemplate: /etc/vpsie-autoscaler/cloud-init.yaml
  sshKeyIds: []  # Optional SSH key IDs
  defaultOSImage: ubuntu-22.04
  tokenTTL: 24h
  joinTimeout: 10m

metrics:
  enabled: true
  port: 8080
  path: /metrics

logging:
  level: info  # debug, info, warn, error
  format: json  # json or console
  output: stdout
```

### 4. Update main.go to load config
```go
func main() {
    configPath := flag.String("config", "/etc/vpsie-autoscaler/config.yaml", "Path to configuration file")
    flag.Parse()

    cfg, err := config.Load(*configPath)
    if err != nil {
        log.Fatal("Failed to load config:", err)
    }

    // Use cfg throughout application
}
```

**Success Criteria:**
- ✅ All 9 TODOs replaced with configuration
- ✅ Configuration hot-reload for non-critical settings
- ✅ Environment variable overrides work
- ✅ Validation catches invalid configs
- ✅ Defaults for all optional values
- ✅ Well-documented config template

---

## PROMPT 5: Cost Optimization Implementation

**Task:** Implement intelligent instance selection and cost optimization to reduce cloud spending by 30-40%.

### Implementation Details:

1. **Create pkg/vpsie/cost/calculator.go** - Pricing calculations
2. **Create pkg/vpsie/cost/optimizer.go** - Instance selection algorithms
3. **Create pkg/rebalancer/rebalancer.go** - Cluster rebalancing
4. **Implement bin packing algorithm** - Maximize resource utilization
5. **Add cost tracking metrics** - Monitor spending trends

### Key Algorithms:
- **First Fit Decreasing** for bin packing
- **Best Fit** for instance selection
- **Rebalancing** based on utilization patterns

---

## PROMPT 6: Enhanced Observability

**Task:** Add production-grade observability with distributed tracing, detailed metrics, and dashboards.

### Implementation:
1. **OpenTelemetry integration** - Distributed tracing
2. **Enhanced metrics** - P50/P95/P99 latencies
3. **Grafana dashboards** - JSON templates
4. **Prometheus alerts** - Critical conditions
5. **Structured events** - Detailed logging

---

## PROMPT 7: E2E Testing Suite

**Task:** Create comprehensive end-to-end tests for production validation.

### Test Scenarios:
1. **Complete scaling lifecycle** - Up and down
2. **Failure recovery** - Controller crashes, API failures
3. **Performance testing** - 1000 node clusters
4. **Chaos engineering** - Network partitions, resource exhaustion
5. **Security validation** - RBAC, secrets, network policies

---

## PROMPT 8: Production Documentation

**Task:** Create complete documentation for operators and developers.

### Documentation Structure:
1. **docs/production/** - Installation, configuration, operations
2. **docs/api/** - Complete API reference
3. **docs/runbooks/** - Troubleshooting guides
4. **docs/architecture/** - System design docs

---

## PROMPT 9: Security Hardening

**Task:** Implement enterprise security requirements.

### Security Features:
1. **Admission webhooks** - Configuration validation
2. **Secret rotation** - Automatic credential refresh
3. **mTLS** - Encrypted communications
4. **Audit logging** - Compliance tracking
5. **Vulnerability scanning** - CI/CD integration

---

## PROMPT 10: Release Automation

**Task:** Prepare for v1.0.0 production release.

### Release Components:
1. **Semantic versioning** - Automated version bumps
2. **Changelog generation** - From commit messages
3. **Release artifacts** - Docker images, Helm charts
4. **Performance benchmarks** - Baseline metrics
5. **Migration guides** - Upgrade procedures

---

## Execution Instructions

### For Each Prompt:
1. Read the complete prompt before starting
2. Implement all required files
3. Add comprehensive tests (target 90% coverage)
4. Update documentation
5. Verify integration with existing code
6. Run integration tests

### Priority Order:
**Week 1-2:** Prompts 1-4 (Critical blockers)
**Week 3:** Prompts 5-6 (Optimization)
**Week 4:** Prompts 7-8 (Testing & Docs)
**Week 5:** Prompts 9-10 (Security & Release)

### Success Metrics:
- All 10 prompts implemented
- 90% test coverage achieved
- Zero critical vulnerabilities
- Performance benchmarks met
- Production deployment successful

---

## Questions to Resolve Before Starting

1. **VPSie API Integration:**
   - Do we have production API credentials for testing?
   - What are the API rate limits in production?
   - How do we handle API maintenance windows?

2. **Cluster Configuration:**
   - What Kubernetes versions must we support?
   - Which CNI plugins are in use?
   - Are there specific security policies?

3. **Performance Requirements:**
   - Maximum acceptable scale-up time?
   - How many nodes should we support?
   - What's the budget constraint?

4. **Deployment Environment:**
   - Air-gapped installations required?
   - Multi-cluster support needed?
   - Specific compliance requirements?

---

## Support Resources

- GitHub Issues: https://github.com/vpsie/vpsie-k8s-autoscaler/issues
- VPSie API Docs: https://api-docs.vpsie.com/
- Kubernetes Autoscaler SIG: https://github.com/kubernetes/autoscaler
- Controller Runtime Book: https://book.kubebuilder.io/

---

END OF PROMPTS - Ready for v1.0.0 Implementation